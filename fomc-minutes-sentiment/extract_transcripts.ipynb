{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Data from FOMC Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import the necessary packages:\n",
    "\n",
    "- `PyPDF2` allows me to get text from PDF files, page by page.\n",
    "- `os` provides tools to interact with the filesystem, including reading from files\n",
    "- `re` is Python's implementation of Regular Expressions, which are useful for extracting information from text\n",
    "- `pandas` is used at the end of this file to export the speaker-content data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import fitz as pymupdf\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I configure the folder where to look for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'pdfs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDFs created by the Federal Reserve contain unicode characters. Since these complicate some of the RegEx operations I want to do later on, I replace them at this stage. Some of these I replace with a rough equivalent, while others I replace with a \"landmark\" so I can still find them and use them in my Regular expressions before removing them from the string later on. I define RegEx patterns to find these characters and then a strategic replacement for each of them. I use this mapping in the `process_page` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_map = {\n",
    "    '\\\\\\\\N\\{EN DASH\\}': '-', \n",
    "    '\\\\\\\\N\\{SUPERSCRIPT [A-Z]*\\}': '<SUPERSCRIPT>', \n",
    "    '\\\\\\\\N\\{EM DASH\\}': '--', \n",
    "    '\\\\\\\\N\\{RIGHT SINGLE QUOTATION MARK\\}': '\\'',\n",
    "    '\\\\\\\\N\\{(?:LEFT|RIGHT) DOUBLE QUOTATION MARK}': '\"',\n",
    "    '_': ' '\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes raw text, splitting a long string on indications of which person is talking. For example, it takes a string like this\n",
    "```\n",
    "'CHAIRMAN GREENSPAN. Good morning everyone. SEVERAL. Good morning.'\n",
    "```\n",
    "and turns it into the following array\n",
    "```\n",
    "['CHAIRMAN GREENSPAN.', 'Good morning everyone.', 'SEVERAL.', 'Good morning.']\n",
    "```\n",
    "In addition, this function also removes (most) page numbers, date headers, and footnotes. There are some of these that slip through the cracks, most of which I am able to identify and remove in later steps of the extraction process. In the end, this function returns an array of strings, which I process further in later functions, especially `extract_data_from_pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page(raw_text):\n",
    "    # Re-encode the raw text as an ASCII string, replacing any unicode characters with a string like \\\\N{SYMBOL NAME GOES HERE}\n",
    "    raw_text = raw_text.encode('ascii', 'namereplace').decode('ascii')\n",
    "    # according to the map defined above, replace these unicode characters\n",
    "    for matchstr, sub in unicode_map.items():\n",
    "        raw_text = re.sub(matchstr, sub, raw_text)\n",
    "\n",
    "    # Split the text on the \"speaker\" indicators as described in the comment above, so I can later label text with the speaker who said it\n",
    "    split = re.split('((?:MR\\.|MRS\\.|MS\\.|(?:VICE )?CHAIR(?:MAN)?) [A-Z]{2,}\\.|SEVERAL(?:\\(\\?\\))?\\.|SPEAKER ?\\(\\?\\)\\.|\\n|PARTICIPANTS?\\.|END OF MEETING)', raw_text)\n",
    "\n",
    "    final = []\n",
    "    # Separate page numbers into their own strings so they can be more easily removed\n",
    "    for item in split:\n",
    "        final.extend(re.split('([0-9]{1,3} of [0-9]{1,3})', item))\n",
    "\n",
    "    # remove empty strings following the split\n",
    "    final = [item.strip() for item in final if item.strip() != '']\n",
    "\n",
    "    trimmed = []\n",
    "    for i, item in enumerate(final):\n",
    "        if i < 3:\n",
    "            # Remove the page number and date usually found at the beginning of a page\n",
    "            if re.match('[0-9]{1,3} of [0-9]{1,3}', item):\n",
    "                continue\n",
    "            elif re.match('[A-Z][a-z]{,8} [1-3]?[0-9](?:-(?:[A-Z][a-z]{,8} )?[1-3]?[0-9])?,? [0-9]{4}', item):\n",
    "                continue\n",
    "        # remove footnote prefixes\n",
    "        if not re.match('[0-9]*  (.*)', item) is None:\n",
    "            trimmed.append(re.match('[0-9]*  (.*)', item).group(1))\n",
    "        else:\n",
    "            trimmed.append(item)\n",
    "\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to figure out if I still need this, or if it should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dates(array):\n",
    "    # array = [item.strip() for item in array if item.strip() != '']\n",
    "    array = [item for item in array if re.match('[0-1]?[0-9]/[0-3]?[0-9]', item) is None]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOMC transcripts are prefixed with some information that is not important for my use case, such as who was in attendance and each person's affiliations. I want to skip over this information. So I identify some \"landmarks\" for different document formats and only extract the information after these \"landmarks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_pdf(fname):\n",
    "    with pymupdf.open(os.path.join('pdfs', fname)) as pages:\n",
    "        include = False\n",
    "        main_arr = []\n",
    "        for page in pages:\n",
    "            extracted = extract_page(page.get_text())\n",
    "            if len(extracted) == 0:\n",
    "                continue\n",
    "            if not (re.match('Transcript of (?:the )?Federal', extracted[0].strip()) is None) or (len(extracted) >= 2 and 'the Federal Open Market Committee' in extracted[1].strip()):\n",
    "                include = True\n",
    "            # Dealing with inconsistent formatting on 12/12/2012 (No \"Transcript of the FOMC\" at the start)\n",
    "            if 'December 11 Session' == extracted[0]:\n",
    "                include = True\n",
    "            if include == False:\n",
    "                print('skipping')\n",
    "                continue\n",
    "            main_arr.extend(drop_dates(extracted))\n",
    "        return main_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utternaces(sep_array):\n",
    "    utterances = []\n",
    "    i = 0\n",
    "    tagged = []\n",
    "    while i < len(sep_array) - 1:\n",
    "        # print(main_arr[i])\n",
    "        is_speaker = not re.match('((?:MR\\.|MRS\\.|MS\\.|(?:VICE )?CHAIR(?:MAN)?) [A-Z]{2,}\\.|SEVERAL(?:\\(\\?\\))?\\.|SPEAKER ?\\(\\?\\)\\.|PARTICIPANTS?\\.)', sep_array[i]) is None\n",
    "        tagged.append({'is_speaker': is_speaker, 'content': sep_array[i]})\n",
    "        i += 1\n",
    "    # print(tagged)\n",
    "    # print(main_arr)\n",
    "\n",
    "    ind = 0\n",
    "    while ind < len(tagged) - 1:\n",
    "        j = ind + 1\n",
    "        if tagged[ind]['is_speaker']:\n",
    "            # print(tagged[ind])\n",
    "            # print(tagged[j])\n",
    "            # print(tagged[ind + 1])\n",
    "            while j < len(tagged) and not tagged[j]['is_speaker']:\n",
    "                # print('not_speak')\n",
    "                j += 1\n",
    "            combined_content = [item['content'] for item in tagged[ind + 1:j]]\n",
    "            # print(combined_content)\n",
    "            speaker = tagged[ind]['content']\n",
    "            processed_content = [item for item in combined_content if item != '']\n",
    "            utterance = {'speaker': speaker, 'content': processed_content}\n",
    "            utterances.append(utterance)\n",
    "        ind = j\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_transcript = extract_data_from_pdf('FOMC20171213meeting.pdf')\n",
    "utterances = get_utternaces(other_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index(utterance, i):\n",
    "    # print(utterance)\n",
    "    utterance['index'] = i\n",
    "    return utterance\n",
    "utterances = [add_index(utterance, i) for i, utterance in enumerate(utterances)]\n",
    "utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_content(content_arr):\n",
    "    # print(len(content_arr))\n",
    "    final_arr = []\n",
    "    include = True\n",
    "    for item in content_arr:\n",
    "        if '(appendix ' in item:\n",
    "            continue\n",
    "        if 'END OF MEETING' in item:\n",
    "            include = False\n",
    "        if include:\n",
    "            final_arr.append(item)\n",
    "    concatenated = ' '.join(final_arr)\n",
    "\n",
    "    date_page_patt = '[A-Z][a-z]{2,8} [1-3]?[0-9],? [0-9]{4} [0-9]{1,3} of [0-9]{1,3}'\n",
    "    return re.sub(date_page_patt, ' ', concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(fname):\n",
    "    year = fname[4:8]\n",
    "    month = fname[8:10]\n",
    "    mday = fname[10:12]\n",
    "    transcript_type = fname[12:].split('.')[0]\n",
    "    datestr = f'{year}-{month}-{mday}'\n",
    "    raw_extract = extract_data_from_pdf(fname)\n",
    "    utterances = get_utternaces(raw_extract)\n",
    "    utterances = [add_index(utterance, i) for i, utterance in enumerate(utterances)]\n",
    "    for utterance in utterances:\n",
    "        utterance['condensed'] = condense_content(utterance['content'])\n",
    "        utterance['date'] = datestr\n",
    "        utterance['type'] = transcript_type\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in os.listdir('pdfs'):\n",
    "    # if i == 40:\n",
    "        # break\n",
    "    print(file)\n",
    "    records.extend(process_pdf(file))\n",
    "    i += 1\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "df.to_csv('test1.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "MR. FISCHER.                519\n",
       "MR. WILLIAMS.               512\n",
       "MR. LOCKHART.               507\n",
       "MR. ENGLISH.                507\n",
       "MR. REINHART.               470\n",
       "VICE CHAIRMAN MCDONOUGH.    469\n",
       "MR. MOSKOW.                 459\n",
       "MS. YELLEN.                 447\n",
       "VICE CHAIRMAN GEITHNER.     443\n",
       "MS. JOHNSON.                443\n",
       "MR. PARRY.                  443\n",
       "MR. KOS.                    380\n",
       "MR. DUDLEY.                 356\n",
       "MR. STERN.                  348\n",
       "MR. KAMIN.                  342\n",
       "MR. MADIGAN.                322\n",
       "MR. GRAMLICH.               318\n",
       "MS. PIANALTO.               304\n",
       "MS. DUKE.                   274\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.speaker.value_counts()[21:40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
