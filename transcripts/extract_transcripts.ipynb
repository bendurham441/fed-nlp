{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Data from FOMC Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import the necessary packages:\n",
    "\n",
    "- `pymupdf` allows me to get text from PDF files, page by page. I started off trying to use `PyPDF2` but found this to be less reliable. Specifically, it would add spaces into extracted text, sometimes in the middle of words. This has the potential to throw off much of the tokenization process.\n",
    "- `os` provides tools to interact with the filesystem, including reading from files\n",
    "- `re` is Python's implementation of Regular Expressions, which are useful for extracting information from text\n",
    "- `pandas` is used at the end of this file to export the speaker-content data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz as pymupdf\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I configure the folder where to look for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'pdfs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDFs created by the Federal Reserve contain unicode characters. Since these complicate some of the RegEx operations I want to do later on, I replace them at this stage. Some of these I replace with a rough equivalent, while others I replace with a \"landmark\" so I can still find them and use them in my Regular expressions before removing them from the string later on. I define RegEx patterns to find these characters and then a strategic replacement for each of them. I use this mapping in the `process_page` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes raw text, splitting a long string on indications of which person is talking. For example, it takes a string like this\n",
    "```\n",
    "'CHAIRMAN GREENSPAN. Good morning everyone. SEVERAL. Good morning.'\n",
    "```\n",
    "and turns it into the following array\n",
    "```\n",
    "['CHAIRMAN GREENSPAN.', 'Good morning everyone.', 'SEVERAL.', 'Good morning.']\n",
    "```\n",
    "In addition, this function also removes (most) page numbers, date headers, and footnotes. There are some of these that slip through the cracks, most of which I am able to identify and remove in later steps of the extraction process. In the end, this function returns an array of strings, which I process further in later functions, especially `extract_data_from_pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_speech = ['[Extended applause]', \n",
    "              '[Meeting break]', \n",
    "              '[No audible response]',\n",
    "              '[No comment indicated]', \n",
    "              '[Aside to Governor Fischer]', \n",
    "              '[Singing and applause]', \n",
    "              '[Chorus of agreement]', \n",
    "              '[Brief recess]', \n",
    "              '[Luncheon recess]', \n",
    "              '[pointing]', \n",
    "              '[Laughter and applause]', \n",
    "              '[Simultaneous speakers]', \n",
    "              '[Show of hands]', \n",
    "              '[gesture]', \n",
    "              '[Singing]', \n",
    "              '[Chorus of ayes]', \n",
    "              '[No response]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_map = {\n",
    "    '\\\\\\\\N\\{EN DASH\\}': '-',\n",
    "    '\\\\\\\\N\\{SUPERSCRIPT [A-Z]*\\}': '<SUPERSCRIPT>',\n",
    "    '\\\\\\\\N\\{EM DASH\\}': '--',\n",
    "    '\\\\\\\\N\\{(?:RIGHT|LEFT) SINGLE QUOTATION MARK\\}': '\\'',\n",
    "    '\\\\\\\\N\\{(?:LEFT|RIGHT) DOUBLE QUOTATION MARK\\}': '\"',\n",
    "    '_': ' ',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION THREE QUARTERS\\}': '3/4',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE QUARTER\\}': '1/4',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE HALF\\}': '1/2',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE THIRD\\}': '1/3',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION TWO THIRDS\\}': '2/3',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION FIVE EIGHTHS\\}': '5/8',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION THREE EIGHTHS\\}': '3/8',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION SEVEN EIGHTHS\\}': '7/8',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE EIGHTH\\}': '1/8',\n",
    "    '\\\\\\\\N\\{SOFT HYPHEN\\}': '',\n",
    "    '\\\\\\\\N\\{EURO SIGN\\}': ' euros',\n",
    "    '\\\\\\\\N\\{YEN SIGN\\}': 'yen',\n",
    "    '\\\\\\\\N\\{(?:DOUBLE )?PRIME\\}': '',\n",
    "    '\\\\\\\\N\\{POUND SIGN\\}': 'pounds',\n",
    "    '\\\\\\\\N\\{HORIZONTAL ELIPSIS\\}': '...',\n",
    "    '\\\\\\\\N\\{MATHEMATICAL ITALIC SMALL PI\\}': 'pi',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER A WITH .*\\}': 'a',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER C WITH .*\\}': 'c',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER E WITH .*\\}': 'e',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER I WITH .*\\}': 'i',\n",
    "    '\\\\\\\\N\\{LATIN (?:SMALL|CAPITAL) LETTER O WITH .*\\}': 'o',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER U WITH .*\\}': 'u',\n",
    "    '\\\\\\\\N\\{LATIN (?:SMALL|CAPITAL) LETTER S WITH .*\\}': 's',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER N WITH .*\\}': 'n',\n",
    "    '\\\\\\\\N\\{GREEK SMALL LETTER .*\\}': 'pi',\n",
    "    '\\\\\\\\N\\{HORIZONTAL ELLIPSIS\\}': '...',\n",
    "    '\\\\\\\\N\\{DAGGER\\}': '',\n",
    "    '\\\\\\\\N\\{MINUS SIGN\\}': '-',\n",
    "    '\\\\\\\\N\\{FIGURE DASH\\}': '-',\n",
    "    '\\\\\\\\N\\{MODIFIER LETTER PRIME}': '\\'',\n",
    "    '\\\\\\\\N\\{NO-BREAK SPACE\\}': ' ',\n",
    "    '\\\\\\\\N\\{HORIZONTAL BAR\\}': '-',\n",
    "    '\\\\\\\\N\\{ACUTE ACCENT\\}': '\\'',\n",
    "    '\\\\\\\\N\\{MICRO SIGN\\}': 'mu',\n",
    "    '\\\\\\\\N\\{REGISTERED SIGN\\}': '',\n",
    "    '\\\\\\\\N\\{BULLET\\}': '',\n",
    "    '\\\\\\\\N\\{MATHEMATICAL ITALIC SMALL [A-Z]*\\}': '',\n",
    "    '\\\\\\\\N\\{ASTERISK OPERATOR\\}': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_speech_pats = ['\\[(L|l)aught?er\\.?\\]', \n",
    "              '\\[(R|r)ecess\\]', \n",
    "              '\\[(P|p)ause\\]',\n",
    "              '\\[(A|a)pplause\\]',\n",
    "              '\\[(L|l)unch (B|b)reak\\]', \n",
    "              '\\[(L|l)unch (R|r)ecess\\]', \n",
    "              '\\[(C|c)offee (B|b)reak\\]', \n",
    "              '\\[(N|n)o (R|r)esponse\\]', \n",
    "              '\\[(S|s)how (O|o)f (H|h)ands\\]', \n",
    "              '\\[No response\\.?\\]',\n",
    "              '\\[Meeting recess(ed)?(?: for lunch)?\\]', \n",
    "              '\\[Simultaneous conversation\\]', \n",
    "              '\\[Chorus of ayes\\]', \n",
    "              '\\[Dessert break\\]', \n",
    "              '\\[(L|a)ughter (A|a)nd (A|a)pplause\\]', \n",
    "              '\\[Extended applause\\]', \n",
    "              '\\[Break\\]', \n",
    "              '\\[Show of hands\\]', \n",
    "              '\\[Short break\\]', \n",
    "              '\\[Silence\\]'\n",
    "              '\\[Laughter, followed by applause\\.?\\]',\n",
    "              '\\[Laughter and extended applause\\.?\\]'\n",
    "              '\\[(U|u)nintelligible\\]',\n",
    "              '\\[(S|s)tatements?--(S|s)ee (A|a)ppendix\\.?\\]'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_nse_process(input_string):\n",
    "        result = input_string\n",
    "        for matchstr, sub in unicode_map.items():\n",
    "            result = re.sub(matchstr, sub, result)\n",
    "        for matchstr in non_speech_pats:\n",
    "            result = re.sub(matchstr, '', result)\n",
    "        for string in non_speech:\n",
    "            result = result.replace(string, '')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_page_remove(arr):\n",
    "    result = []\n",
    "    for i, item in enumerate(arr):\n",
    "        if i < 3 or i > len(arr) - 3:\n",
    "            if not re.match('[0-9]{1,2}\\/[0-9]{1,2}(?:-[0-9]{1,2})?\\/[0-9]{2}$', item) is None:\n",
    "                continue\n",
    "            if not re.match('[A-Z][a-z]{2,8} [0-9]{1,2}(?:-[0-9]{1,2})?,? [0-9]{4}$', item) is None:\n",
    "                continue\n",
    "            if not re.match('[0-9]{1,3} of [0-9]{1,3}$', item) is None:\n",
    "                continue\n",
    "            if not re.match('-?[0-9]{1,3}-?$', item) is None:\n",
    "                continue\n",
    "            result.append(item)\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page(raw_text):\n",
    "    # Re-encode the raw text as an ASCII string, replacing any unicode characters with a string like \\\\N{SYMBOL NAME GOES HERE}\n",
    "    raw_text = unicode_nse_process(raw_text.encode('ascii', 'namereplace').decode('ascii'))\n",
    "\n",
    "    # Split the text on the \"speaker\" indicators as described in the comment above, so I can later label text with the speaker who said it\n",
    "    split = re.split(\n",
    "        '((?:MR\\.|MRS\\.|MS\\.|(?:VICE )?CHAIR(?:MAN)?) [A-Z]{2,}\\.|MR\\. D\\. LINDSEY\\.|SEVERAL(?:\\(\\?\\))?\\.|SPEAKER ?\\(\\?\\)\\.|\\n|PARTICIPANTS?\\.|END OF MEETING)', raw_text)\n",
    "\n",
    "    # remove empty strings following the split\n",
    "    final = [item.strip() for item in split if item.strip() != '']\n",
    "    final = date_page_remove(final)\n",
    "\n",
    "    trimmed = []\n",
    "    for i, item in enumerate(final):\n",
    "        if not re.match('[0-9]*  (.*)', item) is None:\n",
    "            trimmed.append(re.match('[0-9]*  (.*)', item).group(1))\n",
    "        else:\n",
    "            trimmed.append(item)\n",
    "\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOMC transcripts are prefixed with some information that is not important for my use case, such as who was in attendance and each person's affiliations. I want to skip over this information. So I identify some \"landmarks\" for different document formats and only extract the information after these \"landmarks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_pdf(fname):\n",
    "    with pymupdf.open(os.path.join(folder, fname)) as pages:\n",
    "        include = False\n",
    "        main_arr = []\n",
    "        for page in pages:\n",
    "            extracted = extract_page(page.get_text())\n",
    "            if len(extracted) == 0:\n",
    "                continue\n",
    "            if not (re.match('Transcript of (?:the )?Federal', extracted[0].strip()) is None) or (len(extracted) >= 2 and 'the Federal Open Market Committee' in extracted[1].strip()):\n",
    "                include = True\n",
    "            # Dealing with inconsistent formatting on 12/12/2012 (No \"Transcript of the FOMC\" at the start)\n",
    "            if 'December 11 Session' == extracted[0]:\n",
    "                include = True\n",
    "            if include == False:\n",
    "                print('skipping')\n",
    "                continue\n",
    "            main_arr.extend(extracted)\n",
    "        return main_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `extract_page` method creates the array format described above, I want to use this format to create a series of records that looks more like this:\n",
    "```\n",
    "[\n",
    "    {'speaker': 'CHAIRMAN GREENSPAN', 'text': 'Good morning everyone.'},\n",
    "    {'speaker': 'SEVERAL', 'text': 'Good morning.'}\n",
    "]\n",
    "```\n",
    "This is the \"utterance\" format described in Shapiro and Wilson (2021). The `get_utterance` serves this purpose. It first tags each item in the initial array according to whether the item corresponds to a speaker (using a RegEx pattern) and then \"walks\" through the array such that when it finds an item tagged as speaker, it then finds all of the entries in the array before the next item tagged as a speaker and then assigns these as the content of the previously identified speaker. This may seem overly complex, as often the format created above will only have one entry corresponding to one speaker, but it is important to use this more flexible approach since speakers can have more than one item in the array that make up their utterance, such as if the utterance spans multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utternaces(sep_array):\n",
    "    utterances = []\n",
    "    i = 0\n",
    "    tagged = []\n",
    "    while i < len(sep_array) - 1:\n",
    "        is_speaker = not re.match(\n",
    "            '((?:MR\\.|MRS\\.|MS\\.|(?:VICE )?CHAIR(?:MAN)?) (?:[A-Z]\\. )?[A-Z]{2,}\\.|SEVERAL(?:\\(\\?\\))?\\.|SPEAKER ?\\(\\?\\)\\.|PARTICIPANTS?\\.)', sep_array[i]) is None\n",
    "        tagged.append({'is_speaker': is_speaker, 'content': sep_array[i]})\n",
    "        i += 1\n",
    "\n",
    "    ind = 0\n",
    "    while ind < len(tagged) - 1:\n",
    "        j = ind + 1\n",
    "        if tagged[ind]['is_speaker']:\n",
    "            while j < len(tagged) and not tagged[j]['is_speaker']:\n",
    "                j += 1\n",
    "            combined_content = [item['content'] for item in tagged[ind + 1:j]]\n",
    "            speaker = tagged[ind]['content']\n",
    "            processed_content = [\n",
    "                item for item in combined_content if item != '']\n",
    "            utterance = {'speaker': speaker, 'content': processed_content}\n",
    "            utterances.append(utterance)\n",
    "        ind = j\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make a method here to add an index key to each utterance dictionary, which I use later on. The thought here is that I might eventually want to take advantage of the ordering of statements, which is what this index is meant to accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index(utterance, i):\n",
    "    utterance['index'] = i\n",
    "    return utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially if the utterance content contains multiple text items, I need to condense it into one string to be able to tokenize it later. That is what I do here, with a few more clean-up steps to remove the non-informational content of the transcripts such as appendix references, end of meeting indicators, and page numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_content(content_arr):\n",
    "    final_arr = []\n",
    "    include = True\n",
    "    for item in content_arr:\n",
    "        if '(appendix ' in item:\n",
    "            continue\n",
    "        if 'END OF MEETING' in item:\n",
    "            include = False\n",
    "        if include:\n",
    "            final_arr.append(item)\n",
    "    concatenated = ' '.join(final_arr)\n",
    "\n",
    "    return unicode_nse_process(concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier on in the `extract_page` method, I chose to try to include unicode characters in case they contained meaningful content, mostly just to be flexible and to allow myself or others to use these in the future. Here I end up replacing or removing most of those so they don't show up as tokens later on.\n",
    "\n",
    "I also address what I call \"non-speech events\" (NSEs) here. For example, the transcripts sometimes contain strings like \"[Laughter]\" or \"[Applause].\" These do no contribute to the topic (but maybe they contribute to the sentiment, although I think that is beyond the scope of what I am doing here), so I choose to remove them, at least for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(fname):\n",
    "    year = fname[4:8]\n",
    "    month = fname[8:10]\n",
    "    mday = fname[10:12]\n",
    "    transcript_type = fname[12:].split('.')[0]\n",
    "    datestr = f'{year}-{month}-{mday}'\n",
    "    raw_extract = extract_data_from_pdf(fname)\n",
    "    utterances = get_utternaces(raw_extract)\n",
    "    utterances = [add_index(utterance, i)\n",
    "                  for i, utterance in enumerate(utterances)]\n",
    "    for utterance in utterances:\n",
    "        if not isinstance(utterance['content'], collections.abc.Sequence) and math.isnan(utterance['content']):\n",
    "            continue\n",
    "        content = condense_content(utterance['content'])\n",
    "        # according to the map defined above, replace these unicode characters\n",
    "        utterance['content'] = content\n",
    "        utterance['date'] = datestr\n",
    "        utterance['type'] = transcript_type\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in os.listdir('pdfs'):\n",
    "    print(file)\n",
    "    records.extend(process_pdf(file))\n",
    "    i += 1\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "df.to_csv('transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_sents(fname):\n",
    "    year = fname[4:8]\n",
    "    month = fname[8:10]\n",
    "    mday = fname[10:12]\n",
    "    transcript_type = fname[12:].split('.')[0]\n",
    "    datestr = f'{year}-{month}-{mday}'\n",
    "    raw_extract = extract_data_from_pdf(fname)\n",
    "    utterances = get_utternaces(raw_extract)\n",
    "    sentences = []\n",
    "    doc_metadata = {'date': datestr, 'type': transcript_type}\n",
    "    for utterance in utterances:\n",
    "        utter_sents = sent_tokenize(condense_content(utterance['content']))\n",
    "        sentences.extend([{'content': unicode_nse_process(sent), 'speaker': utterance['speaker'],\n",
    "                         **doc_metadata} for sent in utter_sents])\n",
    "    sentences = [add_index(sentence, i)\n",
    "                 for i, sentence in enumerate(sentences)]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in os.listdir('pdfs'):\n",
    "    print(file)\n",
    "    records.extend(process_pdf_sents(file))\n",
    "    i += 1\n",
    "\n",
    "df_sents = pd.DataFrame.from_records(records)\n",
    "\n",
    "df_sents.to_csv('transcripts_sents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify unicode symbols and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    result = re.search('\\\\\\\\N{.*\\}', row['condensed'])\n",
    "    if not result is None:\n",
    "        if result.group() in symbols:\n",
    "            symbols[result.group()] += 1\n",
    "        else:\n",
    "            symbols[result.group()] = 1\n",
    "\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess prevalence of unicode symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for symbol, freq in symbols.items():\n",
    "    total += freq\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[w]': 1,\n",
       " '[a]': 1,\n",
       " '[earlier and gradual]': 1,\n",
       " '[T]': 1,\n",
       " '[System Open Market Account]': 1,\n",
       " '[Research and Statistics]': 2,\n",
       " '[personal consumption expenditures]': 5,\n",
       " '[European Central Bank]': 5,\n",
       " '[non accelerating inflation rate of unemployment]': 1,\n",
       " '[Congressional Budget Office]': 1,\n",
       " '[Japanese government bond]': 1,\n",
       " '[Institute for Supply Management]': 2,\n",
       " '[MFP]': 1,\n",
       " '[West Texas intermediate oil]': 1,\n",
       " '[industrial production]': 2,\n",
       " '[American Economic Association]': 2,\n",
       " '[information technology]': 4,\n",
       " '[equipment and software]': 3,\n",
       " '[alternative minimum tax]': 1,\n",
       " '[probability density function]': 1,\n",
       " '[NIPA]': 1,\n",
       " '[metropolitan statistical areas]': 1,\n",
       " '[Bureau of Labor Statistics]': 3,\n",
       " '[Employment Cost Index]': 4,\n",
       " '[Survey of Professional Forecasters]': 1,\n",
       " '[West Texas Intermediate]': 1,\n",
       " '[the]': 1,\n",
       " '[and]': 1,\n",
       " '[of a return to 2 percent]': 1,\n",
       " '[domestic]': 1,\n",
       " '[English]': 1,\n",
       " '[our]': 1,\n",
       " '[European Monetary Union]': 1,\n",
       " '[Bureau of Economic Analysis]': 3,\n",
       " '[national income and product accounts]': 2,\n",
       " '[commercial and industrial]': 3,\n",
       " '[exhibit 6]': 1,\n",
       " '[Quantitative Impact Study]': 1,\n",
       " '[real business cycle]': 1,\n",
       " '[of MBS]': 1,\n",
       " '[Commodity Research Bureau]': 1,\n",
       " '[t]': 1,\n",
       " '[in]': 1,\n",
       " '[even]': 1,\n",
       " '[generally]': 1,\n",
       " '[One aye]': 1,\n",
       " '[about]': 1,\n",
       " '[OFHEO]': 1,\n",
       " '[Mortgage Bankers Association]': 1,\n",
       " '[Federal Housing Administration]': 1,\n",
       " '[Home Mortgage Disclosure Act]': 1,\n",
       " '[collateralized debt obligation]': 1,\n",
       " '[Office of the Comptroller of the Currency]': 2,\n",
       " '[National Federation of Independent Business]': 1,\n",
       " '[unemployment insurance]': 1,\n",
       " '[liquefied natural gas]': 1,\n",
       " '[Commodity Futures Trading Commission]': 1,\n",
       " '[Quantitative Impact Study 4]': 1,\n",
       " '[mortgage backed securities]': 1,\n",
       " '[i]': 1,\n",
       " '[Emerging Markets Bond Index Plus]': 1,\n",
       " '[West Texas intermediate]': 2,\n",
       " '[International Energy Agency]': 1,\n",
       " '[Former Soviet Union]': 1,\n",
       " '[BOJ]': 1,\n",
       " '[BEI]': 1,\n",
       " '[United Auto Workers]': 1,\n",
       " '[home equity lines of credit]': 1,\n",
       " '[commercial real estate]': 1}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_speech_events = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    result = re.search('\\[(\\w| )+\\]', row['content'])\n",
    "    if not result is None:\n",
    "        if result.group() in non_speech_events:\n",
    "            non_speech_events[result.group()] += 1\n",
    "        else:\n",
    "            non_speech_events[result.group()] = 1\n",
    "non_speech_events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
