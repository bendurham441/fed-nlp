{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Data from FOMC Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import the necessary packages:\n",
    "\n",
    "- `pymupdf` allows me to get text from PDF files, page by page. I started off trying to use `PyPDF2` but found this to be less reliable. Specifically, it would add spaces into extracted text, sometimes in the middle of words. This has the potential to throw off much of the tokenization process.\n",
    "- `os` provides tools to interact with the filesystem, including reading from files\n",
    "- `re` is Python's implementation of Regular Expressions, which are useful for extracting information from text\n",
    "- `pandas` is used at the end of this file to export the speaker-content data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz as pymupdf\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I configure the folder where to look for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'pdfs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDFs created by the Federal Reserve contain unicode characters. Since these complicate some of the RegEx operations I want to do later on, I replace them at this stage. Some of these I replace with a rough equivalent, while others I replace with a \"landmark\" so I can still find them and use them in my Regular expressions before removing them from the string later on. I define RegEx patterns to find these characters and then a strategic replacement for each of them. I use this mapping in the `process_page` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_map = {\n",
    "    '\\\\\\\\N\\{EN DASH\\}': '-', \n",
    "    '\\\\\\\\N\\{SUPERSCRIPT [A-Z]*\\}': '<SUPERSCRIPT>', \n",
    "    '\\\\\\\\N\\{EM DASH\\}': '--', \n",
    "    '\\\\\\\\N\\{(?:RIGHT|LEFT) SINGLE QUOTATION MARK\\}': '\\'',\n",
    "    '\\\\\\\\N\\{(?:LEFT|RIGHT) DOUBLE QUOTATION MARK\\}': '\"',\n",
    "    '_': ' ',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION THREE QUARTERS\\}': '3/4',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE QUARTER\\}': '1/4',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE HALF\\}': '1/2',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE THIRD\\}': '1/3',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION TWO THIRDS\\}': '2/3',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION FIVE EIGHTHS\\}': '5/8',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION THREE EIGHTHS\\}': '3/8',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION SEVEN EIGHTHS\\}': '7/8',\n",
    "    '\\\\\\\\N\\{VULGAR FRACTION ONE EIGHTH\\}': '1/8',\n",
    "    '\\\\\\\\N\\{SOFT HYPHEN\\}': '',\n",
    "    '\\\\\\\\N\\{EURO SIGN\\}': ' euros',\n",
    "    '\\\\\\\\N\\{YEN SIGN\\}': 'yen',\n",
    "    '\\\\\\\\N\\{(?:DOUBLE )?PRIME\\}': '',\n",
    "    '\\\\\\\\N\\{POUND SIGN\\}': 'pounds',\n",
    "    '\\\\\\\\N\\{HORIZONTAL ELIPSIS\\}': '...',\n",
    "    '\\\\\\\\N\\{MATHEMATICAL ITALIC SMALL PI\\}': 'pi',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER A WITH .*\\}': 'a',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER C WITH .*\\}': 'c',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER E WITH .*\\}': 'e',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER I WITH .*\\}': 'i',\n",
    "    '\\\\\\\\N\\{LATIN (?:SMALL|CAPITAL) LETTER O WITH .*\\}': 'o',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER U WITH .*\\}': 'u',\n",
    "    '\\\\\\\\N\\{LATIN (?:SMALL|CAPITAL) LETTER S WITH .*\\}': 's',\n",
    "    '\\\\\\\\N\\{LATIN SMALL LETTER N WITH .*\\}': 'n',\n",
    "    '\\\\\\\\N\\{GREEK SMALL LETTER .*\\}': 'pi',\n",
    "    '\\\\\\\\N\\{HORIZONTAL ELLIPSIS\\}': '...',\n",
    "    '\\\\\\\\N\\{DAGGER\\}': '',\n",
    "    '\\\\\\\\N\\{MINUS SIGN\\}': '-'\n",
    "    # '\\\\\\\\N{\\HORI\\}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes raw text, splitting a long string on indications of which person is talking. For example, it takes a string like this\n",
    "```\n",
    "'CHAIRMAN GREENSPAN. Good morning everyone. SEVERAL. Good morning.'\n",
    "```\n",
    "and turns it into the following array\n",
    "```\n",
    "['CHAIRMAN GREENSPAN.', 'Good morning everyone.', 'SEVERAL.', 'Good morning.']\n",
    "```\n",
    "In addition, this function also removes (most) page numbers, date headers, and footnotes. There are some of these that slip through the cracks, most of which I am able to identify and remove in later steps of the extraction process. In the end, this function returns an array of strings, which I process further in later functions, especially `extract_data_from_pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page(raw_text):\n",
    "    # Re-encode the raw text as an ASCII string, replacing any unicode characters with a string like \\\\N{SYMBOL NAME GOES HERE}\n",
    "    raw_text = raw_text.encode('ascii', 'namereplace').decode('ascii')\n",
    "    # according to the map defined above, replace these unicode characters\n",
    "    for matchstr, sub in unicode_map.items():\n",
    "        raw_text = re.sub(matchstr, sub, raw_text)\n",
    "\n",
    "    # Split the text on the \"speaker\" indicators as described in the comment above, so I can later label text with the speaker who said it\n",
    "    split = re.split('((?:MR\\.|MRS\\.|MS\\.|(?:VICE )?CHAIR(?:MAN)?) [A-Z]{2,}\\.|SEVERAL(?:\\(\\?\\))?\\.|SPEAKER ?\\(\\?\\)\\.|\\n|PARTICIPANTS?\\.|END OF MEETING)', raw_text)\n",
    "\n",
    "    final = []\n",
    "    # Separate page numbers into their own strings so they can be more easily removed\n",
    "    for item in split:\n",
    "        final.extend(re.split('([0-9]{1,3} of [0-9]{1,3})', item))\n",
    "\n",
    "    # remove empty strings following the split\n",
    "    final = [item.strip() for item in final if item.strip() != '']\n",
    "\n",
    "    trimmed = []\n",
    "    for i, item in enumerate(final):\n",
    "        if i < 3:\n",
    "            # Remove the page number and date usually found at the beginning of a page\n",
    "            if re.match('[0-9]{1,3} of [0-9]{1,3}', item):\n",
    "                continue\n",
    "            elif re.match('[A-Z][a-z]{,8} [1-3]?[0-9](?:-(?:[A-Z][a-z]{,8} )?[1-3]?[0-9])?,? [0-9]{4}', item):\n",
    "                continue\n",
    "        # remove footnote prefixes\n",
    "        if not re.match('[0-9]*  (.*)', item) is None:\n",
    "            trimmed.append(re.match('[0-9]*  (.*)', item).group(1))\n",
    "        else:\n",
    "            trimmed.append(item)\n",
    "\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to figure out if I still need this, or if it should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dates(array):\n",
    "    # array = [item.strip() for item in array if item.strip() != '']\n",
    "    array = [item for item in array if re.match('[0-1]?[0-9]/[0-3]?[0-9]', item) is None]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOMC transcripts are prefixed with some information that is not important for my use case, such as who was in attendance and each person's affiliations. I want to skip over this information. So I identify some \"landmarks\" for different document formats and only extract the information after these \"landmarks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_pdf(fname):\n",
    "    with pymupdf.open(os.path.join(folder, fname)) as pages:\n",
    "        include = False\n",
    "        main_arr = []\n",
    "        for page in pages:\n",
    "            extracted = extract_page(page.get_text())\n",
    "            if len(extracted) == 0:\n",
    "                continue\n",
    "            if not (re.match('Transcript of (?:the )?Federal', extracted[0].strip()) is None) or (len(extracted) >= 2 and 'the Federal Open Market Committee' in extracted[1].strip()):\n",
    "                include = True\n",
    "            # Dealing with inconsistent formatting on 12/12/2012 (No \"Transcript of the FOMC\" at the start)\n",
    "            if 'December 11 Session' == extracted[0]:\n",
    "                include = True\n",
    "            if include == False:\n",
    "                print('skipping')\n",
    "                continue\n",
    "            main_arr.extend(drop_dates(extracted))\n",
    "        return main_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utternaces(sep_array):\n",
    "    utterances = []\n",
    "    i = 0\n",
    "    tagged = []\n",
    "    while i < len(sep_array) - 1:\n",
    "        is_speaker = not re.match('((?:MR\\.|MRS\\.|MS\\.|(?:VICE )?CHAIR(?:MAN)?) [A-Z]{2,}\\.|SEVERAL(?:\\(\\?\\))?\\.|SPEAKER ?\\(\\?\\)\\.|PARTICIPANTS?\\.)', sep_array[i]) is None\n",
    "        tagged.append({'is_speaker': is_speaker, 'content': sep_array[i]})\n",
    "        i += 1\n",
    "\n",
    "    ind = 0\n",
    "    while ind < len(tagged) - 1:\n",
    "        j = ind + 1\n",
    "        if tagged[ind]['is_speaker']:\n",
    "            while j < len(tagged) and not tagged[j]['is_speaker']:\n",
    "                j += 1\n",
    "            combined_content = [item['content'] for item in tagged[ind + 1:j]]\n",
    "            speaker = tagged[ind]['content']\n",
    "            processed_content = [item for item in combined_content if item != '']\n",
    "            utterance = {'speaker': speaker, 'content': processed_content}\n",
    "            utterances.append(utterance)\n",
    "        ind = j\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index(utterance, i):\n",
    "    utterance['index'] = i\n",
    "    return utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_content(content_arr):\n",
    "    final_arr = []\n",
    "    include = True\n",
    "    for item in content_arr:\n",
    "        if '(appendix ' in item:\n",
    "            continue\n",
    "        if 'END OF MEETING' in item:\n",
    "            include = False\n",
    "        if include:\n",
    "            final_arr.append(item)\n",
    "    concatenated = ' '.join(final_arr)\n",
    "\n",
    "    date_page_patt = '[A-Z][a-z]{2,8} [1-3]?[0-9],? [0-9]{4} [0-9]{1,3} of [0-9]{1,3}'\n",
    "    return re.sub(date_page_patt, ' ', concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(fname):\n",
    "    year = fname[4:8]\n",
    "    month = fname[8:10]\n",
    "    mday = fname[10:12]\n",
    "    transcript_type = fname[12:].split('.')[0]\n",
    "    datestr = f'{year}-{month}-{mday}'\n",
    "    raw_extract = extract_data_from_pdf(fname)\n",
    "    utterances = get_utternaces(raw_extract)\n",
    "    utterances = [add_index(utterance, i) for i, utterance in enumerate(utterances)]\n",
    "    for utterance in utterances:\n",
    "        utterance['condensed'] = condense_content(utterance['content'])\n",
    "        utterance['date'] = datestr\n",
    "        utterance['type'] = transcript_type\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in os.listdir('pdfs'):\n",
    "    print(file)\n",
    "    records.extend(process_pdf(file))\n",
    "    i += 1\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "df.to_csv('transcripts.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify unicode symbols and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\\\N{HORIZONTAL BAR}': 3,\n",
       " '\\\\N{MODIFIER LETTER PRIME}': 4,\n",
       " '\\\\N{FIGURE DASH}': 7,\n",
       " '\\\\N{MATHEMATICAL ITALIC SMALL R}': 1,\n",
       " '\\\\N{ACUTE ACCENT}': 2,\n",
       " '\\\\N{MICRO SIGN}': 1,\n",
       " '\\\\N{REGISTERED SIGN}': 1,\n",
       " '\\\\N{BULLET}': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    result = re.search('\\\\\\\\N{(?:[A-Z]| )*}', row['condensed'])\n",
    "    if not result is None:\n",
    "        if result.group() in symbols:\n",
    "            symbols[result.group()] += 1\n",
    "        else:\n",
    "            symbols[result.group()] = 1\n",
    "\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess prevalence of unicode symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for symbol, freq in symbols.items():\n",
    "    total += freq\n",
    "total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
