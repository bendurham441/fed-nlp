{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the Transcript PDFs from the Federal Reserve Website\n",
    "\n",
    "In this notebook, I download the transcripts from the Federal Reserve Website so I can use them in the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.federalreserve.gov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I use web scraping to scan through the pages containing historical materials for each year to get the links for each of the transcripts (and conference calls, though I do not use those)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minute_links(year):\n",
    "    url = f'{base_url}/monetarypolicy/fomchistorical{year}.htm'\n",
    "    response = requests.get(url).text\n",
    "    soup = BeautifulSoup(response)\n",
    "    panels = soup.find_all('div', class_='panel')\n",
    "    links = []\n",
    "    for panel in panels:\n",
    "        transcript_element = panel(text=re.compile(r'Transcript'))\n",
    "        if transcript_element:\n",
    "            anchor = transcript_element[0].parent\n",
    "            links.append(f'{base_url}{anchor[\"href\"]}')\n",
    "    print(len(links))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986\n",
      "8\n",
      "1987\n",
      "10\n",
      "1988\n",
      "12\n",
      "1989\n",
      "14\n",
      "1990\n",
      "12\n",
      "1991\n",
      "19\n",
      "1992\n",
      "12\n",
      "1993\n",
      "16\n",
      "1994\n",
      "13\n",
      "1995\n",
      "11\n",
      "1996\n",
      "8\n",
      "1997\n",
      "8\n",
      "1998\n",
      "10\n",
      "1999\n",
      "8\n",
      "2000\n",
      "8\n",
      "2001\n",
      "13\n",
      "2002\n",
      "8\n",
      "2003\n",
      "13\n",
      "2004\n",
      "8\n",
      "2005\n",
      "8\n",
      "2006\n",
      "8\n",
      "2007\n",
      "11\n",
      "2008\n",
      "14\n",
      "2009\n",
      "11\n",
      "2010\n",
      "10\n",
      "2011\n",
      "10\n",
      "2012\n",
      "8\n",
      "2013\n",
      "9\n",
      "2014\n",
      "9\n",
      "2015\n",
      "8\n",
      "2016\n",
      "8\n",
      "2017\n",
      "8\n",
      "2018\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "all_links = []\n",
    "\n",
    "for i in range(1986, 2019):\n",
    "    time.sleep(3 + random.random()* 2)\n",
    "    print(i)\n",
    "    all_links += get_minute_links(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting all the links to the PDFs, I download them one by one (with an added delay so as not to overwhelm the server with many requests) and save them into a folder called \"pdfs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/341 documents completed.\n",
      "10/341 documents completed.\n",
      "20/341 documents completed.\n",
      "30/341 documents completed.\n",
      "40/341 documents completed.\n",
      "50/341 documents completed.\n",
      "60/341 documents completed.\n",
      "70/341 documents completed.\n",
      "80/341 documents completed.\n",
      "90/341 documents completed.\n",
      "100/341 documents completed.\n",
      "110/341 documents completed.\n",
      "120/341 documents completed.\n",
      "130/341 documents completed.\n",
      "140/341 documents completed.\n",
      "150/341 documents completed.\n",
      "160/341 documents completed.\n",
      "170/341 documents completed.\n",
      "180/341 documents completed.\n",
      "190/341 documents completed.\n",
      "200/341 documents completed.\n",
      "210/341 documents completed.\n",
      "220/341 documents completed.\n",
      "230/341 documents completed.\n",
      "240/341 documents completed.\n",
      "250/341 documents completed.\n",
      "260/341 documents completed.\n",
      "270/341 documents completed.\n",
      "280/341 documents completed.\n",
      "290/341 documents completed.\n",
      "300/341 documents completed.\n",
      "310/341 documents completed.\n",
      "320/341 documents completed.\n",
      "330/341 documents completed.\n",
      "340/341 documents completed.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('pdfs', exist_ok=True)\n",
    "\n",
    "for i, item in enumerate(all_links):\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i}/{len(all_links)} documents completed.')\n",
    "    fname = item.split('/')[-1]\n",
    "    # skip any documents that were previously downloaded\n",
    "    if os.path.exists(os.path.join('pdfs', fname)):\n",
    "        continue\n",
    "    response = requests.get(item)\n",
    "    with open(os.path.join('pdfs', fname), 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    time.sleep(5 + 5 * random.random())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
